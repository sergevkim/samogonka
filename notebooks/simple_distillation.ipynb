{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations einops pytorch_lightning wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: False\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(f'CUDA: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import numpy as np\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        subset: Subset,\n",
    "        transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.subset[idx]\n",
    "        # be careful! image is not a numpy array\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = np.array(image)\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed[\"image\"]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10DataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = './data/',\n",
    "        batch_size: int = 8,\n",
    "        num_workers: int = 4,\n",
    "        shuffle: bool = False,\n",
    "        train_transforms: Optional[Callable] = None,\n",
    "        val_transforms: Optional[Callable] = None,\n",
    "        test_transforms: Optional[Callable] = None,\n",
    "        val_size: float = 0.25,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle = shuffle\n",
    "        self.train_transforms = train_transforms\n",
    "        self.val_transforms = val_transforms\n",
    "        self.test_transforms = test_transforms\n",
    "        self.val_size = val_size\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        Path(self.data_dir).mkdir(parents=True, exist_ok=True)\n",
    "        CIFAR10(root=self.data_dir, train=True, download=True)\n",
    "        CIFAR10(root=self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        if stage == 'fit' or stage is None:\n",
    "            trainval_dataset = CIFAR10(root=self.data_dir, train=True)\n",
    "            train_indices, val_indices = train_test_split(\n",
    "                np.arange(len(trainval_dataset)),\n",
    "                test_size=self.val_size,\n",
    "            )\n",
    "            train_subset = Subset(trainval_dataset, train_indices)\n",
    "            val_subset = Subset(trainval_dataset, val_indices)\n",
    "\n",
    "            train_transforms = self.default_transforms() \\\n",
    "                if self.train_transforms is None else self.train_transforms\n",
    "            val_transforms = self.default_transforms() \\\n",
    "                if self.val_transforms is None else self.val_transforms\n",
    "            self.train_dataset = \\\n",
    "                CIFAR10Dataset(train_subset, transform=train_transforms)\n",
    "            self.val_dataset = \\\n",
    "                CIFAR10Dataset(val_subset, transform=val_transforms)\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            test_transforms = self.default_transforms() \\\n",
    "                if self.test_transforms is None else self.test_transforms\n",
    "            self.test_dataset = CIFAR10(\n",
    "                root=self.data_dir,\n",
    "                train=False,\n",
    "                transform=test_transforms,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def default_transforms(self) -> Callable:\n",
    "        return A.Compose([\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops.layers.torch import Rearrange\n",
    "from torchvision.models import resnet18, resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50Model(nn.Module):\n",
    "    def __init__(self, pretrained: bool = False):\n",
    "        super().__init__()\n",
    "        weights = None if pretrained is True else ResNet50_Weights.IMAGENET1K_V1\n",
    "        model = resnet50(weights=weights)\n",
    "        self.feature_extractor = nn.Sequential(*(list(model.children())[:-2]))\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "            Rearrange('bs c 1 1 -> bs c'),\n",
    "            nn.Linear(in_features=2048, out_features=10, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, inputs: str = 'images'):\n",
    "        features = None\n",
    "        if inputs == 'images':\n",
    "            features = self.extract_features(x)\n",
    "        elif inputs == 'features':\n",
    "            features = x\n",
    "\n",
    "        outputs = self.head(features)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18Model(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        model = resnet18(weights=None)\n",
    "        self.feature_extractor = nn.Sequential(*(list(model.children())[:-2]))\n",
    "        self.neck = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=2048, kernel_size=1),\n",
    "            nn.BatchNorm2d(2048),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "            Rearrange('bs c 1 1 -> bs c'),\n",
    "            nn.Linear(in_features=2048, out_features=10, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, inputs: str = 'images'):\n",
    "        features = None\n",
    "        if inputs == 'images':\n",
    "            features = self.extract_features(x)\n",
    "        elif inputs == 'features':\n",
    "            features = x\n",
    "\n",
    "        outputs = self.head(features)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.neck(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "student = ResNet18Model()\n",
    "teacher = ResNet50Model()\n",
    "x = torch.randn(8, 3, 224, 224)\n",
    "assert student.extract_features(x).shape == teacher.extract_features(x).shape\n",
    "assert teacher(x).shape == student(x).shape\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch import Tensor\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModule(LightningModule):\n",
    "    def __init__(self, model: nn.Module, learning_rate: float = 3e-4) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy_metric = Accuracy()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def _step(self, batch: Any, batch_idx: int) -> Dict[str, Tensor]:\n",
    "        images, labels = batch\n",
    "        predicts = self.model(images)\n",
    "        loss = self.criterion(predicts, labels)\n",
    "        accuracy = self.accuracy_metric(predicts, labels)\n",
    "\n",
    "        info = {'loss': loss, 'accuracy': accuracy}\n",
    "\n",
    "        return info\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int) -> Dict[str, Tensor]:\n",
    "        info = self._step(batch, batch_idx)\n",
    "        self.log('train', info)\n",
    "\n",
    "        return info\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int) -> Dict[str, Tensor]:\n",
    "        info = self._step(batch, batch_idx)\n",
    "        self.log('val', info)\n",
    "\n",
    "        return info\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int) -> Dict[str, Tensor]:\n",
    "        info = self._step(batch, batch_idx)\n",
    "        self.log('test', info)\n",
    "\n",
    "        return info\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(params=self.model.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distillation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationModule(LightningModule):\n",
    "    def __init__(self, teacher: nn.Module, student: nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wandb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wandb'"
     ]
    }
   ],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "You want to use `wandb` logger which is not installed yet, install it with `pip install wandb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logger \u001b[38;5;241m=\u001b[39m WandbLogger()\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, logger\u001b[38;5;241m=\u001b[39mlogger, max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet50Model()\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:272\u001b[0m, in \u001b[0;36mWandbLogger.__init__\u001b[0;34m(self, name, save_dir, offline, id, anonymous, version, project, log_model, experiment, prefix, agg_key_funcs, agg_default_func, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    256\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    257\u001b[0m     name: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    270\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m wandb \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m    273\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou want to use `wandb` logger which is not installed yet,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m install it with `pip install wandb`.\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# pragma: no-cover\u001b[39;00m\n\u001b[1;32m    275\u001b[0m         )\n\u001b[1;32m    277\u001b[0m     \u001b[39mif\u001b[39;00m offline \u001b[39mand\u001b[39;00m log_model:\n\u001b[1;32m    278\u001b[0m         \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    279\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProviding log_model=\u001b[39m\u001b[39m{\u001b[39;00mlog_model\u001b[39m}\u001b[39;00m\u001b[39m and offline=\u001b[39m\u001b[39m{\u001b[39;00moffline\u001b[39m}\u001b[39;00m\u001b[39m is an invalid configuration\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m since model checkpoints cannot be uploaded in offline mode.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mHint: Set `offline=False` to log your model.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: You want to use `wandb` logger which is not installed yet, install it with `pip install wandb`."
     ]
    }
   ],
   "source": [
    "model = ResNet50Model()\n",
    "module = ClassificationModule(model=model)\n",
    "datamodule = CIFAR10DataModule()\n",
    "logger = WandbLogger(project='samogonka')\n",
    "trainer = Trainer(accelerator='cpu', logger=logger, max_epochs=30)\n",
    "trainer.fit(module, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m, logger\u001b[38;5;241m=\u001b[39mlogger, log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet18Model()\n\u001b[1;32m      3\u001b[0m module \u001b[38;5;241m=\u001b[39m ClassificationModule(model\u001b[38;5;241m=\u001b[39mmodel)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "model = ResNet18Model()\n",
    "module = ClassificationModule(model=model)\n",
    "datamodule = CIFAR10DataModule(batch_size=4096, num_workers=4)\n",
    "logger = WandbLogger(project='samogonka')\n",
    "trainer = Trainer(accelerator='gpu', logger=logger, log_every_n_steps=2, max_epochs=30)\n",
    "trainer.fit(module, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(accelerator='gpu', logger=logger, log_every_n_steps=2, max_epochs=30)\n",
    "teacher = ResNet50Model()\n",
    "student = ResNet18Model()\n",
    "module = DistillationModule(teacher=teacher, student=student)\n",
    "datamodule = CIFAR10DataModule(batch_size=4096, num_workers=4)\n",
    "trainer.fit(module, datamodule=datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba4ef7194939a0f598f7307e514728c3da9e8572e38d9dd10d8b6f91a1a50201"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
